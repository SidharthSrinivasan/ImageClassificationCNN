{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDVDq4R4cQqN"
      },
      "source": [
        "Import and setup some auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHPwL1QYcQqU"
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import timeit\n",
        "from collections import OrderedDict\n",
        "from pprint import pformat\n",
        "from torch.utils.data.sampler import *\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from google.colab import drive\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "\n",
        "def run(algorithm, dataset_name, filename):\n",
        "    predicted_test_labels, gt_labels, run_time, parameters_count = algorithm(dataset_name)\n",
        "    if predicted_test_labels is None or gt_labels is None:\n",
        "      return (0, 0, 0)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for label, prediction in zip(gt_labels, predicted_test_labels):\n",
        "      total += label.size(0)\n",
        "      correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()   # assuming your model runs on GPU\n",
        "      \n",
        "    accuracy = float(correct) / total\n",
        "    \n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "    return (correct, accuracy, run_time, parameters_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Mjmw05cQq0"
      },
      "source": [
        "def load_data(dataset_name, device, config):\n",
        "    \"\"\"\n",
        "    loads cifar-10 dataset using torchvision, take the last 5k of the training data to be validation data\n",
        "    \"\"\"\n",
        "    CIFAR_training = datasets.CIFAR10(root = './data', train = True, download = True, transform = config['transforms'])\n",
        "    CIFAR_test = datasets.CIFAR10(root = './data', train = False, download = True, transform = config['transforms'])\n",
        "    \n",
        "    training_set = torch.utils.data.Subset(CIFAR_training, list(range(0, 45000)))\n",
        "    validation_set = torch.utils.data.Subset(CIFAR_training, list(range(45000, 50000)))\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(training_set, batch_size = config['batch_size'], shuffle = True)\n",
        "    valid_dataloader = torch.utils.data.DataLoader(validation_set, batch_size = config['batch_size'], shuffle = True)\n",
        "    test_dataloader = torch.utils.data.DataLoader(CIFAR_test ,batch_size=1, shuffle=True)\n",
        "\n",
        "        \n",
        "    return train_dataloader, valid_dataloader, test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2PKEVDSeeJ9"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
        "    nn.init.xavier_uniform_(self.conv1.weight)\n",
        "    self.conv2 = nn.Conv2d(6, 12, kernel_size=5)\n",
        "    self.conv3 = nn.Conv2d(12, 24, kernel_size = 5)\n",
        "    self.fc1 = nn.Linear(384, 64)\n",
        "    self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    upsampler = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = upsampler(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = F.sigmoid(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3GcXjw5emv0"
      },
      "source": [
        "def train(train_dataloader, valid_dataloader, device, config):\n",
        "    model = None\n",
        "    train_losses = []\n",
        "    train_counter = []\n",
        "    validation_losses = []\n",
        "    validation_counter = [i*len(train_dataloader.dataset) for i in range(config['num_epochs'] + 1)]\n",
        "    log_interval = 100\n",
        "\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['lr'],\n",
        "                      momentum=0.5, weight_decay=config['weight_decay'])\n",
        "    \n",
        "    for epoch in range(1, config['num_epochs']+1):\n",
        "      for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
        "            100. * batch_idx / len(train_dataloader), loss.item()))\n",
        "          train_losses.append(loss.item())\n",
        "          train_counter.append(\n",
        "            (batch_idx*64) + ((epoch-1)*len(train_dataloader.dataset)))\n",
        "          \n",
        "      model.eval()\n",
        "      validation_loss = 0\n",
        "      correct = 0\n",
        "      with torch.no_grad():\n",
        "        for data, target in valid_dataloader:\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          output = model(data)\n",
        "          validation_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "          pred = output.data.max(1, keepdim=True)[1]\n",
        "          correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      validation_loss /= len(valid_dataloader.dataset)\n",
        "      validation_losses.append(validation_loss)\n",
        "      print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            validation_loss, correct, len(valid_dataloader.dataset),\n",
        "            100. * correct / len(valid_dataloader.dataset)))\n",
        "    return model\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9dIUkH6e5T2"
      },
      "source": [
        "def save_model_colab_for_submission(model):  # if you are running on colab\n",
        "  drive.mount('/content/gdrive/', force_remount=True)\n",
        "  \n",
        "  torch.save(model.to(torch.device(\"cpu\")), '/content/gdrive/My Drive/model.pt') \n",
        "  \n",
        "\n",
        "def save_model_local_for_submission(model):  # if you are running on your local machine\n",
        "  torch.save(model.to(torch.device(\"cpu\")), 'model.pt')\n",
        "  \n",
        "def test(model, test_dataloader, device):\n",
        "  test_predictions = []\n",
        "  true_labels = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_dataloader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = model(data)\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      test_predictions.append(pred.cpu())\n",
        "      true_labels.append(target.data.view_as(pred.cpu()))\n",
        "    \n",
        "  test_predictions = torch.cat(test_predictions, axis = 0)\n",
        "  true_labels = torch.cat(true_labels, axis = 0)\n",
        "\n",
        "  return test_predictions, true_labels\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params+=params\n",
        "    # uncomment below codes for your debugging\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "def run_NN(dataset_name):\n",
        "    # set parameters cifar10\n",
        "  config = {\n",
        "        'lr': 0.01,\n",
        "        'num_epochs': 15,\n",
        "        'batch_size': 20,\n",
        "        'num_classes': 10,\n",
        "        'regular_constant': 0,\n",
        "        'weight_decay': 0.001,\n",
        "        'transforms': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) }\n",
        "    \n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  \n",
        "  train_dataloader, valid_dataloader, test_dataloader = load_data(dataset_name, device, config)\n",
        "  \n",
        "  model = train(train_dataloader, valid_dataloader, device, config)\n",
        "  parameters_count = count_parameters(model)\n",
        "\n",
        "  device = torch.device(\"cpu\")\n",
        "  start_time = timeit.default_timer()\n",
        "  assert test_dataloader.batch_size == 1, 'Error: You should use use batch size = 1 for the test loader.'\n",
        "  preds, labels = test(model.to(device), test_dataloader, device)\n",
        "  end_time = timeit.default_timer()\n",
        "  \n",
        "\n",
        "  test_time = (end_time - start_time)\n",
        "  print(\"Total run time of testing the model: \", test_time , \" seconds.\")\n",
        "  \n",
        "  save_model_colab_for_submission(model)\n",
        "  \n",
        "  return preds, labels, test_time, parameters_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNNgL7C7cQq-"
      },
      "source": [
        "Main loop. Run time and total score will be shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf9iL8S_cQrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55b03579-3238-4163-e608-888860582428"
      },
      "source": [
        "# Don't edit this cell\n",
        "def run_on_dataset(dataset_name, filename):\n",
        "\n",
        "    correct_predict, accuracy, run_time, parameters_count = run(run_NN, dataset_name, filename)\n",
        "\n",
        "    result = OrderedDict(\n",
        "                  correct_predict=correct_predict,\n",
        "                  accuracy=accuracy,\n",
        "                  run_time=run_time,\n",
        "                  parameters_count=parameters_count)\n",
        "    return result\n",
        "\n",
        "\n",
        "def main():\n",
        "    filenames = { \"CIFAR10\": \"predictions_cifar10.txt\"}\n",
        "    result_all = OrderedDict()\n",
        "    for dataset_name in [\"CIFAR10\"]:\n",
        "        result_all = run_on_dataset(dataset_name, filenames[dataset_name])\n",
        "    with open('result.txt', 'w') as f:\n",
        "        f.writelines(pformat(result_all, indent=4))\n",
        "    print(\"\\nResult:\\n\", pformat(result_all, indent=4))\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
       "outputs": [
         {
           "output_type": "stream",
           "name": "stdout",
           "text": [
             "Files already downloaded and verified\n",
             "Files already downloaded and verified\n"
           ]
         },
         {
           "output_type": "stream",
           "name": "stderr",
           "text": [
             "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
           ]
         },
         {
           "output_type": "stream",
           "name": "stdout",
           "text": [
             "Train Epoch: 1 [0/45000 (0%)]\tLoss: 2.287263\n",
             "Train Epoch: 1 [2000/45000 (4%)]\tLoss: 2.298413\n",
             "Train Epoch: 1 [4000/45000 (9%)]\tLoss: 2.315721\n",
             "Train Epoch: 1 [6000/45000 (13%)]\tLoss: 2.348367\n",
             "Train Epoch: 1 [8000/45000 (18%)]\tLoss: 2.308371\n",
             "Train Epoch: 1 [10000/45000 (22%)]\tLoss: 2.310966\n",
             "Train Epoch: 1 [12000/45000 (27%)]\tLoss: 2.323767\n",
             "Train Epoch: 1 [14000/45000 (31%)]\tLoss: 2.344409\n",
             "Train Epoch: 1 [16000/45000 (36%)]\tLoss: 2.288195\n",
             "Train Epoch: 1 [18000/45000 (40%)]\tLoss: 2.263333\n",
             "Train Epoch: 1 [20000/45000 (44%)]\tLoss: 2.186069\n",
             "Train Epoch: 1 [22000/45000 (49%)]\tLoss: 2.087403\n",
             "Train Epoch: 1 [24000/45000 (53%)]\tLoss: 2.120604\n",
             "Train Epoch: 1 [26000/45000 (58%)]\tLoss: 2.245763\n",
             "Train Epoch: 1 [28000/45000 (62%)]\tLoss: 2.184762\n",
             "Train Epoch: 1 [30000/45000 (67%)]\tLoss: 2.066006\n",
             "Train Epoch: 1 [32000/45000 (71%)]\tLoss: 1.939059\n",
             "Train Epoch: 1 [34000/45000 (76%)]\tLoss: 2.068529\n",
             "Train Epoch: 1 [36000/45000 (80%)]\tLoss: 2.040155\n",
             "Train Epoch: 1 [38000/45000 (84%)]\tLoss: 1.936622\n",
             "Train Epoch: 1 [40000/45000 (89%)]\tLoss: 2.081066\n",
             "Train Epoch: 1 [42000/45000 (93%)]\tLoss: 1.756063\n",
             "Train Epoch: 1 [44000/45000 (98%)]\tLoss: 2.076603\n",
             "\n",
             "Validation set: Avg. loss: 1.9694, Accuracy: 1350/5000 (27%)\n",
             "\n",
             "Train Epoch: 2 [0/45000 (0%)]\tLoss: 2.316126\n",
             "Train Epoch: 2 [2000/45000 (4%)]\tLoss: 1.865393\n",
             "Train Epoch: 2 [4000/45000 (9%)]\tLoss: 1.776539\n",
             "Train Epoch: 2 [6000/45000 (13%)]\tLoss: 1.790860\n",
             "Train Epoch: 2 [8000/45000 (18%)]\tLoss: 1.630193\n",
             "Train Epoch: 2 [10000/45000 (22%)]\tLoss: 1.918756\n",
             "Train Epoch: 2 [12000/45000 (27%)]\tLoss: 1.897656\n",
             "Train Epoch: 2 [14000/45000 (31%)]\tLoss: 2.114166\n",
             "Train Epoch: 2 [16000/45000 (36%)]\tLoss: 1.674786\n",
             "Train Epoch: 2 [18000/45000 (40%)]\tLoss: 1.669717\n",
             "Train Epoch: 2 [20000/45000 (44%)]\tLoss: 1.738362\n",
             "Train Epoch: 2 [22000/45000 (49%)]\tLoss: 2.127356\n",
             "Train Epoch: 2 [24000/45000 (53%)]\tLoss: 1.587865\n",
             "Train Epoch: 2 [26000/45000 (58%)]\tLoss: 1.774870\n",
             "Train Epoch: 2 [28000/45000 (62%)]\tLoss: 1.856988\n",
             "Train Epoch: 2 [30000/45000 (67%)]\tLoss: 1.530668\n",
             "Train Epoch: 2 [32000/45000 (71%)]\tLoss: 1.853192\n",
             "Train Epoch: 2 [34000/45000 (76%)]\tLoss: 1.683039\n",
             "Train Epoch: 2 [36000/45000 (80%)]\tLoss: 1.591210\n",
             "Train Epoch: 2 [38000/45000 (84%)]\tLoss: 1.400663\n",
             "Train Epoch: 2 [40000/45000 (89%)]\tLoss: 1.687723\n",
             "Train Epoch: 2 [42000/45000 (93%)]\tLoss: 1.650327\n",
             "Train Epoch: 2 [44000/45000 (98%)]\tLoss: 1.804457\n",
             "\n",
             "Validation set: Avg. loss: 1.6076, Accuracy: 1996/5000 (40%)\n",
             "\n",
             "Train Epoch: 3 [0/45000 (0%)]\tLoss: 1.730945\n",
             "Train Epoch: 3 [2000/45000 (4%)]\tLoss: 1.518992\n",
             "Train Epoch: 3 [4000/45000 (9%)]\tLoss: 1.830083\n",
             "Train Epoch: 3 [6000/45000 (13%)]\tLoss: 2.111680\n",
             "Train Epoch: 3 [8000/45000 (18%)]\tLoss: 1.955959\n",
             "Train Epoch: 3 [10000/45000 (22%)]\tLoss: 1.733705\n",
             "Train Epoch: 3 [12000/45000 (27%)]\tLoss: 1.746220\n",
             "Train Epoch: 3 [14000/45000 (31%)]\tLoss: 1.353389\n",
             "Train Epoch: 3 [16000/45000 (36%)]\tLoss: 1.989918\n",
             "Train Epoch: 3 [18000/45000 (40%)]\tLoss: 1.663443\n",
             "Train Epoch: 3 [20000/45000 (44%)]\tLoss: 1.747611\n",
             "Train Epoch: 3 [22000/45000 (49%)]\tLoss: 1.662368\n",
             "Train Epoch: 3 [24000/45000 (53%)]\tLoss: 1.560269\n",
             "Train Epoch: 3 [26000/45000 (58%)]\tLoss: 1.591583\n",
             "Train Epoch: 3 [28000/45000 (62%)]\tLoss: 1.404040\n",
             "Train Epoch: 3 [30000/45000 (67%)]\tLoss: 1.647005\n",
             "Train Epoch: 3 [32000/45000 (71%)]\tLoss: 1.366556\n",
             "Train Epoch: 3 [34000/45000 (76%)]\tLoss: 1.454436\n",
             "Train Epoch: 3 [36000/45000 (80%)]\tLoss: 1.597360\n",
             "Train Epoch: 3 [38000/45000 (84%)]\tLoss: 1.158445\n",
             "Train Epoch: 3 [40000/45000 (89%)]\tLoss: 1.694489\n",
             "Train Epoch: 3 [42000/45000 (93%)]\tLoss: 1.585372\n",
             "Train Epoch: 3 [44000/45000 (98%)]\tLoss: 1.538478\n",
             "\n",
             "Validation set: Avg. loss: 1.4835, Accuracy: 2266/5000 (45%)\n",
             "\n",
             "Train Epoch: 4 [0/45000 (0%)]\tLoss: 1.843152\n",
             "Train Epoch: 4 [2000/45000 (4%)]\tLoss: 1.145139\n",
             "Train Epoch: 4 [4000/45000 (9%)]\tLoss: 1.813278\n",
             "Train Epoch: 4 [6000/45000 (13%)]\tLoss: 1.096531\n",
             "Train Epoch: 4 [8000/45000 (18%)]\tLoss: 1.250318\n",
             "Train Epoch: 4 [10000/45000 (22%)]\tLoss: 1.541473\n",
             "Train Epoch: 4 [12000/45000 (27%)]\tLoss: 1.678286\n",
             "Train Epoch: 4 [14000/45000 (31%)]\tLoss: 1.141221\n",
             "Train Epoch: 4 [16000/45000 (36%)]\tLoss: 1.533046\n",
             "Train Epoch: 4 [18000/45000 (40%)]\tLoss: 1.598731\n",
             "Train Epoch: 4 [20000/45000 (44%)]\tLoss: 1.629473\n",
             "Train Epoch: 4 [22000/45000 (49%)]\tLoss: 1.521258\n",
             "Train Epoch: 4 [24000/45000 (53%)]\tLoss: 1.257832\n",
             "Train Epoch: 4 [26000/45000 (58%)]\tLoss: 1.563355\n",
             "Train Epoch: 4 [28000/45000 (62%)]\tLoss: 1.447313\n",
             "Train Epoch: 4 [30000/45000 (67%)]\tLoss: 1.390622\n",
             "Train Epoch: 4 [32000/45000 (71%)]\tLoss: 1.379276\n",
             "Train Epoch: 4 [34000/45000 (76%)]\tLoss: 1.466910\n",
             "Train Epoch: 4 [36000/45000 (80%)]\tLoss: 1.091964\n",
             "Train Epoch: 4 [38000/45000 (84%)]\tLoss: 1.550990\n",
             "Train Epoch: 4 [40000/45000 (89%)]\tLoss: 1.339533\n",
             "Train Epoch: 4 [42000/45000 (93%)]\tLoss: 1.293884\n",
             "Train Epoch: 4 [44000/45000 (98%)]\tLoss: 1.151295\n",
             "\n",
             "Validation set: Avg. loss: 1.4183, Accuracy: 2449/5000 (49%)\n",
             "\n",
             "Train Epoch: 5 [0/45000 (0%)]\tLoss: 1.228166\n",
             "Train Epoch: 5 [2000/45000 (4%)]\tLoss: 1.597431\n",
             "Train Epoch: 5 [4000/45000 (9%)]\tLoss: 1.001999\n",
             "Train Epoch: 5 [6000/45000 (13%)]\tLoss: 1.210729\n",
             "Train Epoch: 5 [8000/45000 (18%)]\tLoss: 1.011280\n",
             "Train Epoch: 5 [10000/45000 (22%)]\tLoss: 1.425886\n",
             "Train Epoch: 5 [12000/45000 (27%)]\tLoss: 1.115891\n",
             "Train Epoch: 5 [14000/45000 (31%)]\tLoss: 1.108521\n",
             "Train Epoch: 5 [16000/45000 (36%)]\tLoss: 1.494968\n",
             "Train Epoch: 5 [18000/45000 (40%)]\tLoss: 1.289808\n",
             "Train Epoch: 5 [20000/45000 (44%)]\tLoss: 1.251222\n",
             "Train Epoch: 5 [22000/45000 (49%)]\tLoss: 1.325695\n",
             "Train Epoch: 5 [24000/45000 (53%)]\tLoss: 1.659368\n",
             "Train Epoch: 5 [26000/45000 (58%)]\tLoss: 1.429955\n",
             "Train Epoch: 5 [28000/45000 (62%)]\tLoss: 1.397319\n",
             "Train Epoch: 5 [30000/45000 (67%)]\tLoss: 1.467281\n",
             "Train Epoch: 5 [32000/45000 (71%)]\tLoss: 1.544430\n",
             "Train Epoch: 5 [34000/45000 (76%)]\tLoss: 1.454234\n",
             "Train Epoch: 5 [36000/45000 (80%)]\tLoss: 1.371337\n",
             "Train Epoch: 5 [38000/45000 (84%)]\tLoss: 1.301281\n",
             "Train Epoch: 5 [40000/45000 (89%)]\tLoss: 1.794924\n",
             "Train Epoch: 5 [42000/45000 (93%)]\tLoss: 1.159627\n",
             "Train Epoch: 5 [44000/45000 (98%)]\tLoss: 1.419636\n",
             "\n",
             "Validation set: Avg. loss: 1.3485, Accuracy: 2571/5000 (51%)\n",
             "\n",
             "Train Epoch: 6 [0/45000 (0%)]\tLoss: 1.679981\n",
             "Train Epoch: 6 [2000/45000 (4%)]\tLoss: 1.011303\n",
             "Train Epoch: 6 [4000/45000 (9%)]\tLoss: 1.551740\n",
             "Train Epoch: 6 [6000/45000 (13%)]\tLoss: 1.840806\n",
             "Train Epoch: 6 [8000/45000 (18%)]\tLoss: 1.149530\n",
             "Train Epoch: 6 [10000/45000 (22%)]\tLoss: 1.795777\n",
             "Train Epoch: 6 [12000/45000 (27%)]\tLoss: 1.420382\n",
             "Train Epoch: 6 [14000/45000 (31%)]\tLoss: 1.598511\n",
             "Train Epoch: 6 [16000/45000 (36%)]\tLoss: 1.231624\n",
             "Train Epoch: 6 [18000/45000 (40%)]\tLoss: 1.613314\n",
             "Train Epoch: 6 [20000/45000 (44%)]\tLoss: 1.365773\n",
             "Train Epoch: 6 [22000/45000 (49%)]\tLoss: 1.107567\n",
             "Train Epoch: 6 [24000/45000 (53%)]\tLoss: 1.158844\n",
             "Train Epoch: 6 [26000/45000 (58%)]\tLoss: 1.173392\n",
             "Train Epoch: 6 [28000/45000 (62%)]\tLoss: 1.337303\n",
             "Train Epoch: 6 [30000/45000 (67%)]\tLoss: 1.133404\n",
             "Train Epoch: 6 [32000/45000 (71%)]\tLoss: 0.906034\n",
             "Train Epoch: 6 [34000/45000 (76%)]\tLoss: 1.166102\n",
             "Train Epoch: 6 [36000/45000 (80%)]\tLoss: 1.531718\n",
             "Train Epoch: 6 [38000/45000 (84%)]\tLoss: 1.082504\n",
             "Train Epoch: 6 [40000/45000 (89%)]\tLoss: 1.021390\n",
             "Train Epoch: 6 [42000/45000 (93%)]\tLoss: 1.310571\n",
             "Train Epoch: 6 [44000/45000 (98%)]\tLoss: 1.414834\n",
             "\n",
             "Validation set: Avg. loss: 1.2942, Accuracy: 2682/5000 (54%)\n",
             "\n",
             "Train Epoch: 7 [0/45000 (0%)]\tLoss: 1.360833\n",
             "Train Epoch: 7 [2000/45000 (4%)]\tLoss: 0.901969\n",
             "Train Epoch: 7 [4000/45000 (9%)]\tLoss: 1.354405\n",
             "Train Epoch: 7 [6000/45000 (13%)]\tLoss: 1.630251\n",
             "Train Epoch: 7 [8000/45000 (18%)]\tLoss: 1.463696\n",
             "Train Epoch: 7 [10000/45000 (22%)]\tLoss: 0.681259\n",
             "Train Epoch: 7 [12000/45000 (27%)]\tLoss: 1.355568\n",
             "Train Epoch: 7 [14000/45000 (31%)]\tLoss: 1.222134\n",
             "Train Epoch: 7 [16000/45000 (36%)]\tLoss: 1.206819\n",
             "Train Epoch: 7 [18000/45000 (40%)]\tLoss: 1.002302\n",
             "Train Epoch: 7 [20000/45000 (44%)]\tLoss: 1.400581\n",
             "Train Epoch: 7 [22000/45000 (49%)]\tLoss: 0.950498\n",
             "Train Epoch: 7 [24000/45000 (53%)]\tLoss: 1.137445\n",
             "Train Epoch: 7 [26000/45000 (58%)]\tLoss: 1.172108\n",
             "Train Epoch: 7 [28000/45000 (62%)]\tLoss: 1.166237\n",
             "Train Epoch: 7 [30000/45000 (67%)]\tLoss: 1.264185\n",
             "Train Epoch: 7 [32000/45000 (71%)]\tLoss: 1.481462\n",
             "Train Epoch: 7 [34000/45000 (76%)]\tLoss: 1.060182\n",
             "Train Epoch: 7 [36000/45000 (80%)]\tLoss: 0.889264\n",
             "Train Epoch: 7 [38000/45000 (84%)]\tLoss: 1.229683\n",
             "Train Epoch: 7 [40000/45000 (89%)]\tLoss: 0.954876\n",
             "Train Epoch: 7 [42000/45000 (93%)]\tLoss: 1.048412\n",
             "Train Epoch: 7 [44000/45000 (98%)]\tLoss: 1.364480\n",
             "\n",
             "Validation set: Avg. loss: 1.2395, Accuracy: 2787/5000 (56%)\n",
             "\n",
             "Train Epoch: 8 [0/45000 (0%)]\tLoss: 0.956097\n",
             "Train Epoch: 8 [2000/45000 (4%)]\tLoss: 1.140003\n",
             "Train Epoch: 8 [4000/45000 (9%)]\tLoss: 1.615586\n",
             "Train Epoch: 8 [6000/45000 (13%)]\tLoss: 1.303907\n",
             "Train Epoch: 8 [8000/45000 (18%)]\tLoss: 1.230798\n",
             "Train Epoch: 8 [10000/45000 (22%)]\tLoss: 1.189542\n",
             "Train Epoch: 8 [12000/45000 (27%)]\tLoss: 1.017509\n",
             "Train Epoch: 8 [14000/45000 (31%)]\tLoss: 1.140048\n",
             "Train Epoch: 8 [16000/45000 (36%)]\tLoss: 1.314264\n",
             "Train Epoch: 8 [18000/45000 (40%)]\tLoss: 1.339389\n",
             "Train Epoch: 8 [20000/45000 (44%)]\tLoss: 0.969100\n",
             "Train Epoch: 8 [22000/45000 (49%)]\tLoss: 1.212949\n",
             "Train Epoch: 8 [24000/45000 (53%)]\tLoss: 0.880959\n",
             "Train Epoch: 8 [26000/45000 (58%)]\tLoss: 1.156297\n",
             "Train Epoch: 8 [28000/45000 (62%)]\tLoss: 1.175113\n",
             "Train Epoch: 8 [30000/45000 (67%)]\tLoss: 0.987497\n",
             "Train Epoch: 8 [32000/45000 (71%)]\tLoss: 1.286949\n",
             "Train Epoch: 8 [34000/45000 (76%)]\tLoss: 1.596116\n",
             "Train Epoch: 8 [36000/45000 (80%)]\tLoss: 1.449784\n",
             "Train Epoch: 8 [38000/45000 (84%)]\tLoss: 0.979061\n",
             "Train Epoch: 8 [40000/45000 (89%)]\tLoss: 1.050689\n",
             "Train Epoch: 8 [42000/45000 (93%)]\tLoss: 0.898303\n",
             "Train Epoch: 8 [44000/45000 (98%)]\tLoss: 1.105382\n",
             "\n",
             "Validation set: Avg. loss: 1.1634, Accuracy: 2931/5000 (59%)\n",
             "\n",
             "Train Epoch: 9 [0/45000 (0%)]\tLoss: 0.910824\n",
             "Train Epoch: 9 [2000/45000 (4%)]\tLoss: 1.513426\n",
             "Train Epoch: 9 [4000/45000 (9%)]\tLoss: 1.354885\n",
             "Train Epoch: 9 [6000/45000 (13%)]\tLoss: 1.534207\n",
             "Train Epoch: 9 [8000/45000 (18%)]\tLoss: 1.268024\n",
             "Train Epoch: 9 [10000/45000 (22%)]\tLoss: 0.933196\n",
             "Train Epoch: 9 [12000/45000 (27%)]\tLoss: 1.227556\n",
             "Train Epoch: 9 [14000/45000 (31%)]\tLoss: 0.827533\n",
             "Train Epoch: 9 [16000/45000 (36%)]\tLoss: 0.977643\n",
             "Train Epoch: 9 [18000/45000 (40%)]\tLoss: 1.432880\n",
             "Train Epoch: 9 [20000/45000 (44%)]\tLoss: 1.319713\n",
             "Train Epoch: 9 [22000/45000 (49%)]\tLoss: 1.445576\n",
             "Train Epoch: 9 [24000/45000 (53%)]\tLoss: 1.095765\n",
             "Train Epoch: 9 [26000/45000 (58%)]\tLoss: 1.511204\n",
             "Train Epoch: 9 [28000/45000 (62%)]\tLoss: 1.182101\n",
             "Train Epoch: 9 [30000/45000 (67%)]\tLoss: 0.922707\n",
             "Train Epoch: 9 [32000/45000 (71%)]\tLoss: 1.064869\n",
             "Train Epoch: 9 [34000/45000 (76%)]\tLoss: 1.112275\n",
             "Train Epoch: 9 [36000/45000 (80%)]\tLoss: 1.160820\n",
             "Train Epoch: 9 [38000/45000 (84%)]\tLoss: 1.431287\n",
             "Train Epoch: 9 [40000/45000 (89%)]\tLoss: 1.152179\n",
             "Train Epoch: 9 [42000/45000 (93%)]\tLoss: 1.168172\n",
             "Train Epoch: 9 [44000/45000 (98%)]\tLoss: 1.360712\n",
             "\n",
             "Validation set: Avg. loss: 1.1177, Accuracy: 3045/5000 (61%)\n",
             "\n",
             "Train Epoch: 10 [0/45000 (0%)]\tLoss: 1.512258\n",
             "Train Epoch: 10 [2000/45000 (4%)]\tLoss: 0.793339\n",
             "Train Epoch: 10 [4000/45000 (9%)]\tLoss: 1.138345\n",
             "Train Epoch: 10 [6000/45000 (13%)]\tLoss: 1.262670\n",
             "Train Epoch: 10 [8000/45000 (18%)]\tLoss: 1.153652\n",
             "Train Epoch: 10 [10000/45000 (22%)]\tLoss: 1.713856\n",
             "Train Epoch: 10 [12000/45000 (27%)]\tLoss: 0.932928\n",
             "Train Epoch: 10 [14000/45000 (31%)]\tLoss: 1.105901\n",
             "Train Epoch: 10 [16000/45000 (36%)]\tLoss: 1.061488\n",
             "Train Epoch: 10 [18000/45000 (40%)]\tLoss: 0.891315\n",
             "Train Epoch: 10 [20000/45000 (44%)]\tLoss: 1.153975\n",
             "Train Epoch: 10 [22000/45000 (49%)]\tLoss: 1.331793\n",
             "Train Epoch: 10 [24000/45000 (53%)]\tLoss: 1.223897\n",
             "Train Epoch: 10 [26000/45000 (58%)]\tLoss: 1.024167\n",
             "Train Epoch: 10 [28000/45000 (62%)]\tLoss: 0.937328\n",
             "Train Epoch: 10 [30000/45000 (67%)]\tLoss: 1.064455\n",
             "Train Epoch: 10 [32000/45000 (71%)]\tLoss: 1.431293\n",
             "Train Epoch: 10 [34000/45000 (76%)]\tLoss: 1.424185\n",
             "Train Epoch: 10 [36000/45000 (80%)]\tLoss: 1.289348\n",
             "Train Epoch: 10 [38000/45000 (84%)]\tLoss: 1.401604\n",
             "Train Epoch: 10 [40000/45000 (89%)]\tLoss: 1.044712\n",
             "Train Epoch: 10 [42000/45000 (93%)]\tLoss: 1.206788\n",
             "Train Epoch: 10 [44000/45000 (98%)]\tLoss: 1.155737\n",
             "\n",
             "Validation set: Avg. loss: 1.1525, Accuracy: 2958/5000 (59%)\n",
             "\n",
             "Train Epoch: 11 [0/45000 (0%)]\tLoss: 0.918444\n",
             "Train Epoch: 11 [2000/45000 (4%)]\tLoss: 1.427064\n",
             "Train Epoch: 11 [4000/45000 (9%)]\tLoss: 1.133873\n",
             "Train Epoch: 11 [6000/45000 (13%)]\tLoss: 0.922397\n",
             "Train Epoch: 11 [8000/45000 (18%)]\tLoss: 1.096937\n",
             "Train Epoch: 11 [10000/45000 (22%)]\tLoss: 1.012845\n",
             "Train Epoch: 11 [12000/45000 (27%)]\tLoss: 1.135049\n",
             "Train Epoch: 11 [14000/45000 (31%)]\tLoss: 0.907649\n",
             "Train Epoch: 11 [16000/45000 (36%)]\tLoss: 0.775554\n",
             "Train Epoch: 11 [18000/45000 (40%)]\tLoss: 1.169134\n",
             "Train Epoch: 11 [20000/45000 (44%)]\tLoss: 1.062730\n",
             "Train Epoch: 11 [22000/45000 (49%)]\tLoss: 1.484638\n",
             "Train Epoch: 11 [24000/45000 (53%)]\tLoss: 0.983224\n",
             "Train Epoch: 11 [26000/45000 (58%)]\tLoss: 1.423502\n",
             "Train Epoch: 11 [28000/45000 (62%)]\tLoss: 1.026214\n",
             "Train Epoch: 11 [30000/45000 (67%)]\tLoss: 1.330651\n",
             "Train Epoch: 11 [32000/45000 (71%)]\tLoss: 0.900355\n",
             "Train Epoch: 11 [34000/45000 (76%)]\tLoss: 1.410302\n",
             "Train Epoch: 11 [36000/45000 (80%)]\tLoss: 0.995322\n",
             "Train Epoch: 11 [38000/45000 (84%)]\tLoss: 0.819312\n",
             "Train Epoch: 11 [40000/45000 (89%)]\tLoss: 0.961988\n",
             "Train Epoch: 11 [42000/45000 (93%)]\tLoss: 1.457336\n",
             "Train Epoch: 11 [44000/45000 (98%)]\tLoss: 0.878774\n",
             "\n",
             "Validation set: Avg. loss: 1.1123, Accuracy: 3056/5000 (61%)\n",
             "\n",
             "Train Epoch: 12 [0/45000 (0%)]\tLoss: 0.681335\n",
             "Train Epoch: 12 [2000/45000 (4%)]\tLoss: 0.966369\n",
             "Train Epoch: 12 [4000/45000 (9%)]\tLoss: 1.031704\n",
             "Train Epoch: 12 [6000/45000 (13%)]\tLoss: 1.614092\n",
             "Train Epoch: 12 [8000/45000 (18%)]\tLoss: 1.115756\n",
             "Train Epoch: 12 [10000/45000 (22%)]\tLoss: 1.020159\n",
             "Train Epoch: 12 [12000/45000 (27%)]\tLoss: 0.830627\n",
             "Train Epoch: 12 [14000/45000 (31%)]\tLoss: 1.045980\n",
             "Train Epoch: 12 [16000/45000 (36%)]\tLoss: 1.438943\n",
             "Train Epoch: 12 [18000/45000 (40%)]\tLoss: 0.887560\n",
             "Train Epoch: 12 [20000/45000 (44%)]\tLoss: 1.563909\n",
             "Train Epoch: 12 [22000/45000 (49%)]\tLoss: 1.154712\n",
             "Train Epoch: 12 [24000/45000 (53%)]\tLoss: 1.450218\n",
             "Train Epoch: 12 [26000/45000 (58%)]\tLoss: 1.513202\n",
             "Train Epoch: 12 [28000/45000 (62%)]\tLoss: 1.192437\n",
             "Train Epoch: 12 [30000/45000 (67%)]\tLoss: 1.307525\n",
             "Train Epoch: 12 [32000/45000 (71%)]\tLoss: 1.193295\n",
             "Train Epoch: 12 [34000/45000 (76%)]\tLoss: 0.861136\n",
             "Train Epoch: 12 [36000/45000 (80%)]\tLoss: 1.078083\n",
             "Train Epoch: 12 [38000/45000 (84%)]\tLoss: 1.019696\n",
             "Train Epoch: 12 [40000/45000 (89%)]\tLoss: 1.160975\n",
             "Train Epoch: 12 [42000/45000 (93%)]\tLoss: 1.542975\n",
             "Train Epoch: 12 [44000/45000 (98%)]\tLoss: 1.225868\n",
             "\n",
             "Validation set: Avg. loss: 1.0919, Accuracy: 3049/5000 (61%)\n",
             "\n",
             "Train Epoch: 13 [0/45000 (0%)]\tLoss: 1.352549\n",
             "Train Epoch: 13 [2000/45000 (4%)]\tLoss: 1.282206\n",
             "Train Epoch: 13 [4000/45000 (9%)]\tLoss: 0.984469\n",
             "Train Epoch: 13 [6000/45000 (13%)]\tLoss: 1.048717\n",
             "Train Epoch: 13 [8000/45000 (18%)]\tLoss: 0.568211\n",
             "Train Epoch: 13 [10000/45000 (22%)]\tLoss: 1.510368\n",
             "Train Epoch: 13 [12000/45000 (27%)]\tLoss: 0.960100\n",
             "Train Epoch: 13 [14000/45000 (31%)]\tLoss: 0.817624\n",
             "Train Epoch: 13 [16000/45000 (36%)]\tLoss: 0.461631\n",
             "Train Epoch: 13 [18000/45000 (40%)]\tLoss: 1.055595\n",
             "Train Epoch: 13 [20000/45000 (44%)]\tLoss: 1.253376\n",
             "Train Epoch: 13 [22000/45000 (49%)]\tLoss: 1.382118\n",
             "Train Epoch: 13 [24000/45000 (53%)]\tLoss: 1.274076\n",
             "Train Epoch: 13 [26000/45000 (58%)]\tLoss: 1.089582\n",
             "Train Epoch: 13 [28000/45000 (62%)]\tLoss: 0.804414\n",
             "Train Epoch: 13 [30000/45000 (67%)]\tLoss: 0.957903\n",
             "Train Epoch: 13 [32000/45000 (71%)]\tLoss: 0.600161\n",
             "Train Epoch: 13 [34000/45000 (76%)]\tLoss: 1.042550\n",
             "Train Epoch: 13 [36000/45000 (80%)]\tLoss: 0.808706\n",
             "Train Epoch: 13 [38000/45000 (84%)]\tLoss: 0.824706\n",
             "Train Epoch: 13 [40000/45000 (89%)]\tLoss: 1.202529\n",
             "Train Epoch: 13 [42000/45000 (93%)]\tLoss: 0.836186\n",
             "Train Epoch: 13 [44000/45000 (98%)]\tLoss: 1.075287\n",
             "\n",
             "Validation set: Avg. loss: 1.1094, Accuracy: 3026/5000 (61%)\n",
             "\n",
             "Train Epoch: 14 [0/45000 (0%)]\tLoss: 0.976232\n",
             "Train Epoch: 14 [2000/45000 (4%)]\tLoss: 1.191947\n",
             "Train Epoch: 14 [4000/45000 (9%)]\tLoss: 1.101075\n",
             "Train Epoch: 14 [6000/45000 (13%)]\tLoss: 1.111918\n",
             "Train Epoch: 14 [8000/45000 (18%)]\tLoss: 0.882401\n",
             "Train Epoch: 14 [10000/45000 (22%)]\tLoss: 0.800197\n",
             "Train Epoch: 14 [12000/45000 (27%)]\tLoss: 1.094823\n"
           ]
         },
         {
           "output_type": "error",
           "ename": "KeyboardInterrupt",
           "evalue": "ignored",
           "traceback": [
             "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
             "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
             "\u001b[0;32m<ipython-input-8-20e0530463c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
             "\u001b[0;32m<ipython-input-8-20e0530463c3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"CIFAR10\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mresult_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
             "\u001b[0;32m<ipython-input-8-20e0530463c3>\u001b[0m in \u001b[0;36mrun_on_dataset\u001b[0;34m(dataset_name, filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_thres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.65\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcorrect_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_NN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_thres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_thres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
             "\u001b[0;32m<ipython-input-1-2fba1e7dfec5>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(algorithm, dataset_name, filename)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mpredicted_test_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpredicted_test_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgt_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
             "\u001b[0;32m<ipython-input-7-b38c743828d3>\u001b[0m in \u001b[0;36mrun_NN\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0mparameters_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
             "\u001b[0;32m<ipython-input-4-04bf7baecd23>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, valid_dataloader, device, config)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
             "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
             "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
             "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
           ]
         }
       ]
     }
   ]
 }
